{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1: Reading data and regression\n",
    "## Due September 5th\n",
    "\n",
    "In class we've been working on developing the tools for linear regression.  In this homework, we'll use those tools to show that one of the original 'big data' problems can be solved quite credibly with ordinary least squares.  Note that this exercise is primarily intended to do two things: 1) provide you with some practice gathering data, and 2) evaluate your ability to reason somewhat abstractly about models.  With respect to the second point, it is in your grade's interest to provide complete and well-reasoned answers to narrative questions posed.  A correct answer can be awarded very few points if its reasoning is absent or unclear, and an incorrect answer can be awarded full points if it is well argued.   \n",
    "\n",
    "Please turn in your work via github classroom.\n",
    "\n",
    "## The big one\n",
    "If you've ever lived in an area prone to seismic activity, you know that people are always a little apprehensive about *the big one*, that giant earthquake (Magnitude greater than 7) that is going to break the pots and collapse the overpasses.  However, the big one rarely happens... until it does.  What happens more frequently is smaller earthquakes.  And even more frequently than that, even smaller earthquakes.  In fact, it's long been understood that earthquake frequency has an inverse relationship with magnitude.  Here, we're going to quantify that relationship for the west coast of the US.  \n",
    "\n",
    "## Data wrangling\n",
    "The first thing that we'll need to do is to aquire a dataset that can help us say something about earthquake frequency.  Fortunately, the United States Geologic Survey keeps such a database.  \n",
    "\n",
    "First, navigate to https://earthquake.usgs.gov/earthquakes/search/.  This is the USGS' central repository for earthquake data.  We'll be interested in data from the last twenty years.  Enter the appropriate date.  Next, we're interested in data from the west coast of the lower 48.  Use the Custom Geographic Region button on the right side of the page, followed by the Draw Rectangle on the Map button.  Draw a rectangle around the west coast, from the Canadian to Mexican border.  Next, open the Output Options tab and select .csv (comma separated values, a plain text format).  Finally, under Limit Results, enter 19999 (the site will throw an error if you make this value bigger).  This series of commands will deliver the ~20k most recent earthquakes of all sizes to occur in this region of the world.  It may take a moment for their server to pull your query together. Download the file.\n",
    "\n",
    "Next, import the data into ipython.  This is easily done with [Pandas' read_csv function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to import the earthquake data file here\n",
    "import pandas as pd\n",
    "earthquakes = pd.read_csv('earthquakes.csv',sep=\",\",engine='python',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interrogate the Pandas dataframe for the available fields using its built in method 'keys'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquakes.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're primarily interested in determining the relationship between relative frequency and magnitude.  Extract the magnitude variable from the pandas data frame using the 'mag' key.  \n",
    "\n",
    "Next, you'll need to determine the number of earthquakes that occured in the data as a function of magnitude.  This is easily done by deciding on a set of bins, and then counting the number of items in each bin.  This is also known as a [histogram](https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html), and is easily computed using either numpy or matplotlib.  You'll want to use a fairly large number of bins, say 50 (equally spaced).  *Note that the histogram function provided by either of the above libraries returns total counts rather than relative frequencies: you'll need to compute relative frequencies by dividing the returned counts by the total number of earthquakes in the dataset.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag = earthquakes['mag']\n",
    "import numpy as np\n",
    "hist, bins = np.histogram(mag, bins=50) # freq\n",
    "relative_hist = hist / len(mag) # relative freq\n",
    "# bin edges (len(hist) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(20 pts) Make a plot with bin centroids on the $x$-axis and relative frequency on the $y$-axis.  Based on your results, please provide a few sentences describing whether the model $y = w_0 + w_1 x$ is a good fit to the data, if $x$ represents earthquake magnitude and $y$ represents relative frequency.**\n",
    "\n",
    "!Answers go here\n",
    "\n",
    "A linear model ($y = w_0 + w_1 x$) will not be a good fit to the data. Based on the plot, as the magnitude of an earthquake increases, the relative frequency seems to decrease exponentially. Therefore, a linear model would not be able to accurately depict the relationship between earthquake magnitude and relative frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# get centroids\n",
    "space = (bins[1] - bins[0]) / 2\n",
    "centroid = np.array([x+space for x in bins][:len(bins)-1])\n",
    "\n",
    "plt.plot(centroid,relative_hist,'ro')\n",
    "plt.xlabel('Magnitude')\n",
    "plt.ylabel('Relative Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, a physical model that gives the predicted relative frequency of earthquakes of different magnitudes can be derived from physics, and is given by a so-called *power-law*\n",
    "$$\n",
    "y = ax^b\n",
    "$$\n",
    "**(20pts) If you wanted to directly fit this power-law model to these data, would you be able to use the linear regression code that we've already developed to do so?  If so, how?  If not, why not?**\n",
    "\n",
    "!Answers go here\n",
    "\n",
    "No you could not use the linear regression code to fit the power-law model to these data. Our current linear regression code is used to find the best fit for a model with a near-constant (linear) slope. The power-law model has a non-constant slope. Specifically for this data, the slope of the plot above (magnitude vs. relative frequency) starts off in a very steep negative direction and decreases in magnitude as earthquake magnitude increases. Our current linear regression code, which seeks to find the best constant slope for data, would not be able to capture this phenomena from a power-law model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, it is not possible to use linear regression to fit this model directly.  However, it can be done by using a clever transformation of the data.  **(20pts) Plot the base-10 logarithm of the event counts.  Now does a linear model seem like a good fit?  If the power law is a good fit to the untransformed data, why does it make sense that a linear model should fit the log-transformed data (HINT: take the logarithm of both sides of the power law)**\n",
    "\n",
    "**(UPDATE!)** I should have noted this before: The earthquake magnitude $x$ has *already* been log-transformed, which is to say that a magnitude 7 is 10 times more powerful than a magnitude 6.  The power law stated above applies to the earthquake strength prior to it having the logarithm taken.  As such, a more reasonable way to state the question would be as follows: A physical model that gives the predicted relative frequency of earthquakes of different magnitudes can be derived from physics and is given by a so-called *power-law*\n",
    "$$\n",
    "y = a P^b,\n",
    "$$\n",
    "where $P=10^x$. Show that taking the logarithm of both sides of this equation leads to a linear relationship between the earthquake magnitude $x$ and the log-frequency $\\text{log}_{10}(y)$. \n",
    "\n",
    "!Answers go here\n",
    "\n",
    "We start with $y = a P^b = a 10^{bx}$.\n",
    "\n",
    "Taking the logarithm of both sides we get $\\text{log}_{10}(y) = \\text{log}_{10}(a 10^{bx})$.\n",
    "\n",
    "Then, using the logarithm product rule we get $\\text{log}_{10}(y) = \\text{log}_{10}(a) + \\text{log}_{10}(10^{bx})$.\n",
    "\n",
    "So, $\\text{log}_{10}(y) = \\text{log}_{10}(a) + bx$.\n",
    "\n",
    "Thus, the log-frequency is linearly related to the earthquake magnitude with a slope of $b$ and y-intercept of $\\text{log}_{10}(a)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot base-10 log of data\n",
    "# use event counts\n",
    "hist_transformed = np.log10(hist+1) # for values with 0\n",
    "plt.plot(centroid,hist_transformed,'ro')\n",
    "plt.xlabel('Magnitude')\n",
    "plt.ylabel('Relative Frequency')\n",
    "plt.show()\n",
    "print(hist)\n",
    "print(hist_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, **(40pts) use one of the methods that you developed for linear regression in class to fit a line to the log-transformed counts.  What is the slope of that line?  Is the model a good fit for all sizes of earthquake?**\n",
    "\n",
    "!Answers go here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(w):\n",
    "    resid = hist_transformed - w[1]*centroid - w[0]\n",
    "    dI_dw0 = -np.sum(resid)\n",
    "    dI_dw1 = -np.sum(resid*centroid)\n",
    "    return np.array([dI_dw0, dI_dw1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([5, -1]) # w0, w1\n",
    "s = 0.0001 # step size\n",
    "tol = 0.001 # grad_I size tolerance\n",
    "grad_I = gradient(w) # initial gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grad_I)\n",
    "while np.linalg.norm(grad_I) > tol:\n",
    "    w = w - s*grad_I\n",
    "    grad_I = gradient(w)\n",
    "print(\"w:\", w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
